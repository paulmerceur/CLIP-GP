# Gaussian Process for CLIP adaptation

## Problem definition

Let us consider a classification problem with **K** classes, each represented by a prototype vector **t<sub>k</sub> âˆˆ â„<sup>d</sup>**, which is expressed as a linear combination of fixed text embeddings  

t<sub>k</sub> = Î£<sub>j=1â€¦M</sub> Î±<sub>k,j</sub> f<sub>j</sub> = **F Î±<sub>k</sub>**

where **F = [fâ‚,â€¦,f_M] âˆˆ â„<sup>dÃ—M</sup>** is the matrix of fixed text embeddings obtained from the CLIP text encoder and **Î±<sub>k</sub> âˆˆ â„<sup>M</sup>** are the unknown balancing weights for class *k*.

**Adaptation.** Given a support set  

S = {(x<sup>(m)</sup>, y<sup>(m)</sup>)}<sub>m=1â€¦K Ã— N</sub>

of *N* images per class and one-hot labels y<sub>i</sub> âˆˆ {0,1}<sup>K</sup>, a pretrained CLIP visual encoder produces visual embeddings **v<sub>i</sub> âˆˆ â„<sup>d</sup>**.  
We learn a linear projection **W âˆˆ â„<sup>dÃ—d</sup>** and compute  

vÌƒ<sub>i</sub> = **W** v<sub>i</sub>.

Logits for class *k*:

â„“<sub>i,k</sub> = vÌƒ<sub>i</sub><sup>âŠ¤</sup> t<sub>k</sub> = vÌƒ<sub>i</sub><sup>âŠ¤</sup>(**F Î±<sub>k</sub>**)

Class probabilities:

Å·<sub>i,k</sub> = exp (â„“<sub>i,k</sub>) / Î£<sub>j=1â€¦K</sub> exp (â„“<sub>i,j</sub>)

Cross-entropy over the support set S:

L<sub>CE</sub>(x<sub>i</sub>, y<sub>i</sub>) = âˆ’Î£<sub>k=1â€¦K</sub> y<sub>i,k</sub> log Å·<sub>i,k</sub>

## Gaussian Process Prior on Prototypes

Model Î±<sub>k</sub>(f) as a latent function over text embeddings f âˆˆ â„<sup>d</sup> with a Gaussian Process prior:

Î±<sub>k</sub>(f) ~ ğ’¢ğ’«(m<sub>k</sub>(f), K<sub>k</sub>(f,fâ€²))

Evaluated at {f<sub>j</sub>}<sub>j=1â€¦M</sub> we obtain

p(Î±<sub>k</sub>) = ğ’©(m<sub>k</sub>, K<sub>k</sub>)  

m<sub>k</sub> = [m<sub>k</sub>(fâ‚),â€¦,m<sub>k</sub>(f_M)]<sup>âŠ¤</sup>,â€ƒ(K<sub>k</sub>)<sub>ij</sub> = K<sub>k</sub>(f<sub>i</sub>, f<sub>j</sub>)

## Posterior Inference

Joint likelihood:

p(y | {Î±<sub>k</sub>}, W,{v<sub>i</sub>}) = Î <sub>i=1â€¦N</sub> exp(y<sub>i</sub><sup>âŠ¤</sup>â„“<sub>i</sub>) / Î£<sub>k=1â€¦K</sub> exp (â„“<sub>i,k</sub>)

Bayes posterior (intractable):

p({Î±<sub>k</sub>} | {(x<sub>i</sub>, y<sub>i</sub>)}, W) âˆ p(y | {Î±<sub>k</sub>}, W,{v<sub>i</sub>}) â‹… Î <sub>k=1â€¦K</sub> p(Î±<sub>k</sub>)

### Variational approximation

Assume q(Î±<sub>k</sub>) = ğ’©(Î¼<sub>k</sub>, Î£<sub>k</sub>) and optimize the ELBO

â„’ = E<sub>q</sub>[log p(y | {Î±<sub>k</sub>}, W)] âˆ’ Î£<sub>k=1â€¦K</sub> KL(q(Î±<sub>k</sub>) â€– p(Î±<sub>k</sub>))

Monte-Carlo approximation with M samples Î±<sub>k</sub><sup>(m)</sup>:

âˆ’â„’ â‰ˆ âˆ’(1/M) Î£<sub>m=1â€¦M</sub> log p(y | {Î±<sub>k</sub><sup>(m)</sup>}, W) + Î² Î£<sub>k=1â€¦K</sub> KL(q(Î±<sub>k</sub>)â€–p(Î±<sub>k</sub>))
