/scratch/pmerceur/CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/pmerceur/CLAP/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: RN50
config_file: configs/trainers/GP_linear_VP.yaml
dataset_config_file: configs/datasets/caltech101.yaml
enhanced_base: none
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
num_templates: 1
opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.ADAPTER.INIT', 'ZS', 'TRAINER.ADAPTER.CONSTRAINT', 'none', 'TRAINER.ADAPTER.NUM_TEMPLATES', '10']
output_dir: output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1
resume: 
root: /scratch/pmerceur/data
seed: 1
source_domains: None
target_domains: None
trainer: ADAPTER
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 128
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 128
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 4
  ROOT: /scratch/pmerceur/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.1
  LR_SCHEDULER: cosine
  MAX_EPOCH: 300
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0
OUTPUT_DIR: output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  ADAPTER:
    CONSTRAINT: none
    ENHANCED_BASE: none
    GP_BETA: 0.1
    GP_KERNEL_TYPE: linear
    GP_LENGTHSCALE: 1.0
    GP_LR: 0.01
    GP_NOISE: 0.0001
    GP_NUM_MC_SAMPLES: 10
    GP_OUTPUTSCALE: 1.0
    GP_UPDATE_FREQ: 1
    GP_USE_DIAGONAL_COV: True
    INIT: ZS
    NUM_TEMPLATES: 10
    PREC: fp16
    USE_GP: True
    USE_VISUAL_PROJECTION: True
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ADAPTER
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Loading trainer: ADAPTER
Loading dataset: Caltech101
Reading split from /scratch/pmerceur/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /scratch/pmerceur/data/caltech-101/split_fewshot/shot_4-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  400
# val      400
# test     2,465
---------  ----------
Loading CLIP (backbone: RN50)
Building custom CLIP
Selected 10 templates for GP weighting:
  0: a photo of a {}.
  1: a toy {}.
  2: art of the {}.
  3: graffiti of a {}.
  4: a photo of one {}.
  5: a bad photo of a {}.
  6: a good photo of a {}.
  7: a rendition of the {}.
  8: the {} in a video game.
  9: a photo of the large {}.
Using Zero-Shot initialization in Linear Probing
GP is active: prototypes are not independent learnable parameters.
Turning off gradients in both the image and the text encoder
  Parameter gp_weighter.weight_temperature requires_grad: True
  Parameter gp_weighter.variational_strategy.base_variational_strategy._variational_distribution.variational_mean requires_grad: True
  Parameter gp_weighter.variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar requires_grad: True
  Parameter gp_weighter.mean_module.raw_constant requires_grad: True
  Parameter gp_weighter.covar_module.raw_outputscale requires_grad: True
  Parameter gp_weighter.covar_module.base_kernel.raw_variance requires_grad: True
  Parameter adapter.visual_projection.weight requires_grad: True
Number of adapter parameters: 2
Number of GP parameters: 6
Adapter parameters:
  logit_scale: torch.Size([])
  visual_projection.weight: torch.Size([1024, 1024])
GP parameters:
  weight_temperature: torch.Size([])
  variational_strategy.base_variational_strategy._variational_distribution.variational_mean: torch.Size([100, 10])
  variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar: torch.Size([100, 10, 10])
  mean_module.raw_constant: torch.Size([100])
  covar_module.raw_outputscale: torch.Size([100])
  covar_module.base_kernel.raw_variance: torch.Size([100, 1, 1])
Loading evaluator: Classification
Extracting features from: test
Zero-Shot accuracy on test: 84.46
Extracting features from: train
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1/tensorboard)
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] Entropy=4.605, mu_std=0.0077
epoch [1/300] batch [1/3] loss 1.2804 (1.2804) acc_train 66.4062 (66.4062) acc_test 83.4888 (83.4888) kl_divergence 0.0005 (0.0005) eta 1:39:35
epoch [1/300] batch [2/3] loss 1.1849 (1.2326) acc_train 71.0938 (68.7500) acc_test 84.6247 (84.0568) kl_divergence 0.0005 (0.0005) eta 0:53:48
epoch [1/300] batch [3/3] loss 1.2657 (1.2436) acc_train 67.1875 (68.2292) acc_test 84.6653 (84.2596) kl_divergence 0.0005 (0.0005) eta 0:36:40
epoch [11/300] batch [1/3] loss 2.1037 (2.1037) acc_train 65.6250 (65.6250) acc_test 75.3753 (75.3753) kl_divergence 0.0188 (0.0188) eta 0:00:47
epoch [11/300] batch [2/3] loss 2.4763 (2.2900) acc_train 71.0938 (68.3594) acc_test 75.2130 (75.2941) kl_divergence 0.0192 (0.0190) eta 0:00:47
epoch [11/300] batch [3/3] loss 2.1403 (2.2401) acc_train 66.4062 (67.7083) acc_test 75.4970 (75.3617) kl_divergence 0.0195 (0.0192) eta 0:00:47
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [21/300] batch [1/3] loss 1.4297 (1.4297) acc_train 82.0312 (82.0312) acc_test 84.2596 (84.2596) kl_divergence 0.0151 (0.0151) eta 0:00:46
epoch [21/300] batch [2/3] loss 0.9506 (1.1901) acc_train 85.9375 (83.9844) acc_test 83.8540 (84.0568) kl_divergence 0.0149 (0.0150) eta 0:00:47
epoch [21/300] batch [3/3] loss 1.5443 (1.3082) acc_train 78.9062 (82.2917) acc_test 83.0426 (83.7187) kl_divergence 0.0146 (0.0149) eta 0:00:46
epoch [31/300] batch [1/3] loss 0.6535 (0.6535) acc_train 88.2812 (88.2812) acc_test 84.2596 (84.2596) kl_divergence 0.0084 (0.0084) eta 0:00:44
epoch [31/300] batch [2/3] loss 0.5341 (0.5938) acc_train 88.2812 (88.2812) acc_test 84.3813 (84.3205) kl_divergence 0.0083 (0.0084) eta 0:00:43
epoch [31/300] batch [3/3] loss 0.4235 (0.5371) acc_train 86.7188 (87.7604) acc_test 84.3813 (84.3408) kl_divergence 0.0081 (0.0083) eta 0:00:44
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [41/300] batch [1/3] loss 0.3637 (0.3637) acc_train 90.6250 (90.6250) acc_test 84.3002 (84.3002) kl_divergence 0.0046 (0.0046) eta 0:00:42
epoch [41/300] batch [2/3] loss 0.2166 (0.2901) acc_train 94.5312 (92.5781) acc_test 83.3266 (83.8134) kl_divergence 0.0045 (0.0046) eta 0:00:43
epoch [41/300] batch [3/3] loss 0.2996 (0.2933) acc_train 92.1875 (92.4479) acc_test 83.6105 (83.7458) kl_divergence 0.0044 (0.0045) eta 0:00:42
epoch [51/300] batch [1/3] loss 0.3631 (0.3631) acc_train 90.6250 (90.6250) acc_test 84.6247 (84.6247) kl_divergence 0.0027 (0.0027) eta 0:00:41
epoch [51/300] batch [2/3] loss 0.2998 (0.3315) acc_train 93.7500 (92.1875) acc_test 85.1521 (84.8884) kl_divergence 0.0026 (0.0026) eta 0:00:41
epoch [51/300] batch [3/3] loss 0.3029 (0.3219) acc_train 92.1875 (92.1875) acc_test 85.1116 (84.9628) kl_divergence 0.0026 (0.0026) eta 0:00:41
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [61/300] batch [1/3] loss 0.2155 (0.2155) acc_train 93.7500 (93.7500) acc_test 84.7465 (84.7465) kl_divergence 0.0016 (0.0016) eta 0:00:38
epoch [61/300] batch [2/3] loss 0.2751 (0.2453) acc_train 91.4062 (92.5781) acc_test 85.2333 (84.9899) kl_divergence 0.0016 (0.0016) eta 0:00:39
epoch [61/300] batch [3/3] loss 0.1945 (0.2284) acc_train 93.7500 (92.9688) acc_test 84.8276 (84.9358) kl_divergence 0.0016 (0.0016) eta 0:00:40
epoch [71/300] batch [1/3] loss 0.2367 (0.2367) acc_train 96.0938 (96.0938) acc_test 84.3813 (84.3813) kl_divergence 0.0010 (0.0010) eta 0:00:40
epoch [71/300] batch [2/3] loss 0.3049 (0.2708) acc_train 92.9688 (94.5312) acc_test 84.8682 (84.6247) kl_divergence 0.0010 (0.0010) eta 0:00:39
epoch [71/300] batch [3/3] loss 0.1824 (0.2413) acc_train 96.8750 (95.3125) acc_test 84.3408 (84.5301) kl_divergence 0.0010 (0.0010) eta 0:00:38
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [81/300] batch [1/3] loss 0.1202 (0.1202) acc_train 96.8750 (96.8750) acc_test 84.3408 (84.3408) kl_divergence 0.0006 (0.0006) eta 0:00:38
epoch [81/300] batch [2/3] loss 0.2409 (0.1806) acc_train 93.7500 (95.3125) acc_test 84.2596 (84.3002) kl_divergence 0.0006 (0.0006) eta 0:00:36
epoch [81/300] batch [3/3] loss 0.2313 (0.1975) acc_train 96.0938 (95.5729) acc_test 84.4219 (84.3408) kl_divergence 0.0006 (0.0006) eta 0:00:36
epoch [91/300] batch [1/3] loss 0.1088 (0.1088) acc_train 96.8750 (96.8750) acc_test 84.7059 (84.7059) kl_divergence 0.0004 (0.0004) eta 0:00:33
epoch [91/300] batch [2/3] loss 0.1588 (0.1338) acc_train 96.0938 (96.4844) acc_test 84.8682 (84.7870) kl_divergence 0.0004 (0.0004) eta 0:00:33
epoch [91/300] batch [3/3] loss 0.1109 (0.1262) acc_train 96.8750 (96.6146) acc_test 84.7870 (84.7870) kl_divergence 0.0004 (0.0004) eta 0:00:34
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [101/300] batch [1/3] loss 0.0923 (0.0923) acc_train 97.6562 (97.6562) acc_test 84.8276 (84.8276) kl_divergence 0.0003 (0.0003) eta 0:00:35
epoch [101/300] batch [2/3] loss 0.1166 (0.1045) acc_train 94.5312 (96.0938) acc_test 84.4625 (84.6450) kl_divergence 0.0003 (0.0003) eta 0:00:33
epoch [101/300] batch [3/3] loss 0.0708 (0.0933) acc_train 97.6562 (96.6146) acc_test 84.9493 (84.7465) kl_divergence 0.0003 (0.0003) eta 0:00:33
epoch [111/300] batch [1/3] loss 0.1039 (0.1039) acc_train 96.8750 (96.8750) acc_test 85.4361 (85.4361) kl_divergence 0.0003 (0.0003) eta 0:00:34
epoch [111/300] batch [2/3] loss 0.1045 (0.1042) acc_train 98.4375 (97.6562) acc_test 85.0710 (85.2536) kl_divergence 0.0003 (0.0003) eta 0:00:32
epoch [111/300] batch [3/3] loss 0.2144 (0.1410) acc_train 95.3125 (96.8750) acc_test 85.3955 (85.3009) kl_divergence 0.0003 (0.0003) eta 0:00:31
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [121/300] batch [1/3] loss 0.0933 (0.0933) acc_train 97.6562 (97.6562) acc_test 84.6653 (84.6653) kl_divergence 0.0002 (0.0002) eta 0:00:30
epoch [121/300] batch [2/3] loss 0.0999 (0.0966) acc_train 96.0938 (96.8750) acc_test 84.5030 (84.5842) kl_divergence 0.0002 (0.0002) eta 0:00:31
epoch [121/300] batch [3/3] loss 0.0325 (0.0753) acc_train 98.4375 (97.3958) acc_test 84.4219 (84.5301) kl_divergence 0.0002 (0.0002) eta 0:00:31
epoch [131/300] batch [1/3] loss 0.0804 (0.0804) acc_train 97.6562 (97.6562) acc_test 85.5984 (85.5984) kl_divergence 0.0002 (0.0002) eta 0:00:28
epoch [131/300] batch [2/3] loss 0.0124 (0.0464) acc_train 100.0000 (98.8281) acc_test 85.0710 (85.3347) kl_divergence 0.0002 (0.0002) eta 0:00:28
epoch [131/300] batch [3/3] loss 0.0518 (0.0482) acc_train 99.2188 (98.9583) acc_test 85.2333 (85.3009) kl_divergence 0.0002 (0.0002) eta 0:00:28
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [141/300] batch [1/3] loss 0.0323 (0.0323) acc_train 99.2188 (99.2188) acc_test 84.6653 (84.6653) kl_divergence 0.0001 (0.0001) eta 0:00:25
epoch [141/300] batch [2/3] loss 0.1112 (0.0717) acc_train 96.0938 (97.6562) acc_test 84.6247 (84.6450) kl_divergence 0.0001 (0.0001) eta 0:00:25
epoch [141/300] batch [3/3] loss 0.0392 (0.0609) acc_train 98.4375 (97.9167) acc_test 84.9493 (84.7465) kl_divergence 0.0001 (0.0001) eta 0:00:25
epoch [151/300] batch [1/3] loss 0.0339 (0.0339) acc_train 99.2188 (99.2188) acc_test 85.1116 (85.1116) kl_divergence 0.0001 (0.0001) eta 0:00:23
epoch [151/300] batch [2/3] loss 0.0496 (0.0418) acc_train 99.2188 (99.2188) acc_test 84.9087 (85.0101) kl_divergence 0.0001 (0.0001) eta 0:00:24
epoch [151/300] batch [3/3] loss 0.0544 (0.0460) acc_train 98.4375 (98.9583) acc_test 84.9493 (84.9899) kl_divergence 0.0001 (0.0001) eta 0:00:23
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [161/300] batch [1/3] loss 0.0482 (0.0482) acc_train 99.2188 (99.2188) acc_test 84.5842 (84.5842) kl_divergence 0.0001 (0.0001) eta 0:00:21
epoch [161/300] batch [2/3] loss 0.0799 (0.0641) acc_train 99.2188 (99.2188) acc_test 84.5436 (84.5639) kl_divergence 0.0001 (0.0001) eta 0:00:22
epoch [161/300] batch [3/3] loss 0.0458 (0.0580) acc_train 99.2188 (99.2188) acc_test 84.3813 (84.5030) kl_divergence 0.0001 (0.0001) eta 0:00:22
epoch [171/300] batch [1/3] loss 0.0332 (0.0332) acc_train 98.4375 (98.4375) acc_test 85.8012 (85.8012) kl_divergence 0.0001 (0.0001) eta 0:00:20
epoch [171/300] batch [2/3] loss 0.0250 (0.0291) acc_train 99.2188 (98.8281) acc_test 85.7607 (85.7809) kl_divergence 0.0001 (0.0001) eta 0:00:21
epoch [171/300] batch [3/3] loss 0.0162 (0.0248) acc_train 100.0000 (99.2188) acc_test 85.5578 (85.7066) kl_divergence 0.0001 (0.0001) eta 0:00:20
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [181/300] batch [1/3] loss 0.0179 (0.0179) acc_train 99.2188 (99.2188) acc_test 85.4767 (85.4767) kl_divergence 0.0001 (0.0001) eta 0:00:20
epoch [181/300] batch [2/3] loss 0.0719 (0.0449) acc_train 96.8750 (98.0469) acc_test 85.5578 (85.5172) kl_divergence 0.0001 (0.0001) eta 0:00:20
epoch [181/300] batch [3/3] loss 0.0057 (0.0318) acc_train 100.0000 (98.6979) acc_test 85.4361 (85.4902) kl_divergence 0.0001 (0.0001) eta 0:00:20
epoch [191/300] batch [1/3] loss 0.0080 (0.0080) acc_train 100.0000 (100.0000) acc_test 85.0710 (85.0710) kl_divergence 0.0001 (0.0001) eta 0:00:17
epoch [191/300] batch [2/3] loss 0.0146 (0.0113) acc_train 100.0000 (100.0000) acc_test 85.0710 (85.0710) kl_divergence 0.0001 (0.0001) eta 0:00:17
epoch [191/300] batch [3/3] loss 0.0060 (0.0095) acc_train 100.0000 (100.0000) acc_test 85.4767 (85.2062) kl_divergence 0.0001 (0.0001) eta 0:00:17
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [201/300] batch [1/3] loss 0.0266 (0.0266) acc_train 99.2188 (99.2188) acc_test 85.4767 (85.4767) kl_divergence 0.0001 (0.0001) eta 0:00:17
epoch [201/300] batch [2/3] loss 0.0168 (0.0217) acc_train 100.0000 (99.6094) acc_test 85.3550 (85.4158) kl_divergence 0.0001 (0.0001) eta 0:00:16
epoch [201/300] batch [3/3] loss 0.0138 (0.0191) acc_train 99.2188 (99.4792) acc_test 85.5172 (85.4496) kl_divergence 0.0001 (0.0001) eta 0:00:16
epoch [211/300] batch [1/3] loss 0.0155 (0.0155) acc_train 100.0000 (100.0000) acc_test 85.3955 (85.3955) kl_divergence 0.0000 (0.0000) eta 0:00:14
epoch [211/300] batch [2/3] loss 0.0125 (0.0140) acc_train 100.0000 (100.0000) acc_test 85.2738 (85.3347) kl_divergence 0.0000 (0.0000) eta 0:00:14
epoch [211/300] batch [3/3] loss 0.0061 (0.0114) acc_train 100.0000 (100.0000) acc_test 85.6795 (85.4496) kl_divergence 0.0001 (0.0000) eta 0:00:14
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [221/300] batch [1/3] loss 0.0216 (0.0216) acc_train 100.0000 (100.0000) acc_test 85.1521 (85.1521) kl_divergence 0.0000 (0.0000) eta 0:00:12
epoch [221/300] batch [2/3] loss 0.0224 (0.0220) acc_train 99.2188 (99.6094) acc_test 85.1927 (85.1724) kl_divergence 0.0000 (0.0000) eta 0:00:13
epoch [221/300] batch [3/3] loss 0.0135 (0.0192) acc_train 100.0000 (99.7396) acc_test 84.9493 (85.0980) kl_divergence 0.0000 (0.0000) eta 0:00:12
epoch [231/300] batch [1/3] loss 0.0100 (0.0100) acc_train 100.0000 (100.0000) acc_test 85.2333 (85.2333) kl_divergence 0.0000 (0.0000) eta 0:00:11
epoch [231/300] batch [2/3] loss 0.0088 (0.0094) acc_train 100.0000 (100.0000) acc_test 84.9493 (85.0913) kl_divergence 0.0000 (0.0000) eta 0:00:11
epoch [231/300] batch [3/3] loss 0.0074 (0.0087) acc_train 100.0000 (100.0000) acc_test 85.3550 (85.1792) kl_divergence 0.0000 (0.0000) eta 0:00:11
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [241/300] batch [1/3] loss 0.0405 (0.0405) acc_train 99.2188 (99.2188) acc_test 85.4361 (85.4361) kl_divergence 0.0000 (0.0000) eta 0:00:09
epoch [241/300] batch [2/3] loss 0.0089 (0.0247) acc_train 100.0000 (99.6094) acc_test 85.8418 (85.6390) kl_divergence 0.0000 (0.0000) eta 0:00:09
epoch [241/300] batch [3/3] loss 0.0250 (0.0248) acc_train 99.2188 (99.4792) acc_test 85.6795 (85.6525) kl_divergence 0.0000 (0.0000) eta 0:00:09
epoch [251/300] batch [1/3] loss 0.0080 (0.0080) acc_train 100.0000 (100.0000) acc_test 85.3550 (85.3550) kl_divergence 0.0000 (0.0000) eta 0:00:08
epoch [251/300] batch [2/3] loss 0.0063 (0.0071) acc_train 100.0000 (100.0000) acc_test 85.3144 (85.3347) kl_divergence 0.0000 (0.0000) eta 0:00:07
epoch [251/300] batch [3/3] loss 0.0058 (0.0067) acc_train 100.0000 (100.0000) acc_test 85.8012 (85.4902) kl_divergence 0.0000 (0.0000) eta 0:00:08
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [261/300] batch [1/3] loss 0.0076 (0.0076) acc_train 100.0000 (100.0000) acc_test 85.3550 (85.3550) kl_divergence 0.0000 (0.0000) eta 0:00:06
epoch [261/300] batch [2/3] loss 0.0170 (0.0123) acc_train 100.0000 (100.0000) acc_test 85.5984 (85.4767) kl_divergence 0.0000 (0.0000) eta 0:00:06
epoch [261/300] batch [3/3] loss 0.0154 (0.0133) acc_train 100.0000 (100.0000) acc_test 85.7607 (85.5713) kl_divergence 0.0000 (0.0000) eta 0:00:06
epoch [271/300] batch [1/3] loss 0.0136 (0.0136) acc_train 100.0000 (100.0000) acc_test 85.7201 (85.7201) kl_divergence 0.0000 (0.0000) eta 0:00:05
epoch [271/300] batch [2/3] loss 0.0114 (0.0125) acc_train 100.0000 (100.0000) acc_test 86.0446 (85.8824) kl_divergence 0.0000 (0.0000) eta 0:00:05
epoch [271/300] batch [3/3] loss 0.0249 (0.0166) acc_train 99.2188 (99.7396) acc_test 85.8824 (85.8824) kl_divergence 0.0000 (0.0000) eta 0:00:05
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
epoch [281/300] batch [1/3] loss 0.0094 (0.0094) acc_train 100.0000 (100.0000) acc_test 85.6795 (85.6795) kl_divergence 0.0000 (0.0000) eta 0:00:03
epoch [281/300] batch [2/3] loss 0.0061 (0.0078) acc_train 100.0000 (100.0000) acc_test 85.6389 (85.6592) kl_divergence 0.0000 (0.0000) eta 0:00:03
epoch [281/300] batch [3/3] loss 0.0083 (0.0079) acc_train 100.0000 (100.0000) acc_test 85.5172 (85.6119) kl_divergence 0.0000 (0.0000) eta 0:00:03
epoch [291/300] batch [1/3] loss 0.0129 (0.0129) acc_train 100.0000 (100.0000) acc_test 85.5172 (85.5172) kl_divergence 0.0000 (0.0000) eta 0:00:01
epoch [291/300] batch [2/3] loss 0.0092 (0.0111) acc_train 100.0000 (100.0000) acc_test 85.7607 (85.6389) kl_divergence 0.0000 (0.0000) eta 0:00:01
epoch [291/300] batch [3/3] loss 0.0098 (0.0106) acc_train 99.2188 (99.7396) acc_test 85.7607 (85.6795) kl_divergence 0.0000 (0.0000) eta 0:00:01
epoch [300/300] batch [1/3] loss 0.0105 (0.0105) acc_train 100.0000 (100.0000) acc_test 85.8418 (85.8418) kl_divergence 0.0000 (0.0000) eta 0:00:00
epoch [300/300] batch [2/3] loss 0.0062 (0.0084) acc_train 100.0000 (100.0000) acc_test 85.5984 (85.7201) kl_divergence 0.0000 (0.0000) eta 0:00:00
epoch [300/300] batch [3/3] loss 0.0101 (0.0090) acc_train 100.0000 (100.0000) acc_test 85.6795 (85.7066) kl_divergence 0.0000 (0.0000) eta 0:00:00
Checkpoint saved to output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1/adapter/model.pth.tar-300
Finish training
Elapsed: 0:01:47
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:05<01:36,  5.05s/it] 10%|█         | 2/20 [00:05<00:39,  2.17s/it] 15%|█▌        | 3/20 [00:05<00:21,  1.25s/it] 20%|██        | 4/20 [00:05<00:13,  1.22it/s] 25%|██▌       | 5/20 [00:05<00:08,  1.73it/s] 30%|███       | 6/20 [00:05<00:06,  2.31it/s] 35%|███▌      | 7/20 [00:05<00:04,  2.94it/s] 40%|████      | 8/20 [00:06<00:03,  3.57it/s] 45%|████▌     | 9/20 [00:06<00:04,  2.53it/s] 50%|█████     | 10/20 [00:06<00:03,  3.13it/s] 55%|█████▌    | 11/20 [00:07<00:02,  3.73it/s] 60%|██████    | 12/20 [00:07<00:01,  4.30it/s] 65%|██████▌   | 13/20 [00:07<00:01,  4.82it/s] 70%|███████   | 14/20 [00:07<00:01,  5.24it/s] 75%|███████▌  | 15/20 [00:07<00:00,  5.60it/s] 80%|████████  | 16/20 [00:07<00:00,  5.88it/s] 85%|████████▌ | 17/20 [00:09<00:01,  1.57it/s] 90%|█████████ | 18/20 [00:09<00:00,  2.04it/s] 95%|█████████▌| 19/20 [00:09<00:00,  2.58it/s]100%|██████████| 20/20 [00:10<00:00,  1.99it/s]
=> result
* total: 2,465
* correct: 2,121
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 78.7%
