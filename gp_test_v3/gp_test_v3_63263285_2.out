/scratch/pmerceur/CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/scratch/pmerceur/CLAP/.venv/lib/python3.11/site-packages/torch/_compile.py:32: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information
  return disable_fn(*args, **kwargs)
/scratch/pmerceur/CLAP/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: RN50
config_file: configs/trainers/GP_linear_VP.yaml
dataset_config_file: configs/datasets/caltech101.yaml
enhanced_base: none
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
num_templates: 1
opts: ['DATASET.NUM_SHOTS', '4', 'TRAINER.ADAPTER.INIT', 'ZS', 'TRAINER.ADAPTER.CONSTRAINT', 'none', 'TRAINER.ADAPTER.NUM_TEMPLATES', '10']
output_dir: output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1
resume: 
root: /scratch/pmerceur/data
seed: 1
source_domains: None
target_domains: None
trainer: ADAPTER
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 128
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 128
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 4
  ROOT: /scratch/pmerceur/data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: RN50
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.1
  LR_SCHEDULER: cosine
  MAX_EPOCH: 100
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0
OUTPUT_DIR: output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  ADAPTER:
    CONSTRAINT: none
    ENHANCED_BASE: none
    GP_BETA: 0.1
    GP_KERNEL_TYPE: linear
    GP_LENGTHSCALE: 1.0
    GP_LR: 0.01
    GP_NOISE: 0.0001
    GP_NUM_MC_SAMPLES: 10
    GP_OUTPUTSCALE: 1.0
    GP_UPDATE_FREQ: 1
    GP_USE_DIAGONAL_COV: True
    INIT: ZS
    NUM_TEMPLATES: 10
    PREC: fp16
    USE_GP: True
    USE_VISUAL_PROJECTION: True
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ADAPTER
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Loading trainer: ADAPTER
Loading dataset: Caltech101
Reading split from /scratch/pmerceur/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /scratch/pmerceur/data/caltech-101/split_fewshot/shot_4-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  400
# val      400
# test     2,465
---------  ----------
Loading CLIP (backbone: RN50)
Building custom CLIP
Selected 10 templates for GP weighting:
  0: a photo of a {}.
  1: a toy {}.
  2: art of the {}.
  3: graffiti of a {}.
  4: a photo of one {}.
  5: a bad photo of a {}.
  6: a good photo of a {}.
  7: a rendition of the {}.
  8: the {} in a video game.
  9: a photo of the large {}.
[INFO] Visual projection frozen (GP active) â€“ prototypes must adapt instead.
Using Zero-Shot initialization in Linear Probing
GP is active: prototypes are not independent learnable parameters.
Turning off gradients in frozen modules and honour pre-set flags
Number of adapter parameters: 2
Adapter parameters:
  logit_scale: torch.Size([])
  visual_projection.weight: torch.Size([1024, 1024])
GP parameters:
  weight_temperature: torch.Size([])
  variational_strategy.base_variational_strategy._variational_distribution.variational_mean: torch.Size([100, 10])
  variational_strategy.base_variational_strategy._variational_distribution.chol_variational_covar: torch.Size([100, 10, 10])
  mean_module.raw_constant: torch.Size([100])
  covar_module.raw_outputscale: torch.Size([100])
  covar_module.base_kernel.raw_variance: torch.Size([100, 1, 1])
Loading evaluator: Classification
Extracting features from: test
Zero-Shot accuracy on test: 84.46
Extracting features from: train
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1/tensorboard)
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] entropy=4.605 | mu_std=0.0077 | temp=1.00 | top idx [58, 74, 96] probs [0.0102, 0.0102, 0.0102]
epoch [1/100] batch [1/3] loss 1.2804 (1.2804) acc_train 66.4062 (66.4062) acc_test 83.4888 (83.4888) kl_divergence 0.0005 (0.0005) eta 0:17:47
epoch [1/100] batch [2/3] loss 1.1849 (1.2326) acc_train 71.0938 (68.7500) acc_test 84.6247 (84.0568) kl_divergence 0.0005 (0.0005) eta 0:09:50
epoch [1/100] batch [3/3] loss 1.2657 (1.2437) acc_train 67.1875 (68.2292) acc_test 84.6653 (84.2596) kl_divergence 0.0005 (0.0005) eta 0:06:36
[INFO][GP] entropy=4.605 | mu_std=0.0077 | temp=1.00 | top idx [58, 74, 96] probs [0.0102, 0.0102, 0.0102]
[INFO][GP] entropy=4.605 | mu_std=0.0076 | temp=1.00 | top idx [58, 74, 96] probs [0.0102, 0.0102, 0.0101]
[INFO][GP] entropy=4.605 | mu_std=0.0076 | temp=0.99 | top idx [58, 74, 15] probs [0.0102, 0.0102, 0.0101]
[INFO][GP] entropy=4.605 | mu_std=0.0076 | temp=0.99 | top idx [74, 58, 15] probs [0.0102, 0.0102, 0.0101]
[INFO][GP] entropy=4.605 | mu_std=0.0078 | temp=0.98 | top idx [74, 58, 1] probs [0.0102, 0.0101, 0.0101]
[INFO][GP] entropy=4.605 | mu_std=0.0080 | temp=0.98 | top idx [1, 74, 58] probs [0.0102, 0.0102, 0.0101]
[INFO][GP] entropy=4.605 | mu_std=0.0084 | temp=0.97 | top idx [1, 0, 74] probs [0.0102, 0.0102, 0.0102]
[INFO][GP] entropy=4.605 | mu_std=0.0089 | temp=0.97 | top idx [1, 0, 12] probs [0.0102, 0.0102, 0.0102]
[INFO][GP] entropy=4.605 | mu_std=0.0095 | temp=0.96 | top idx [1, 0, 12] probs [0.0102, 0.0102, 0.0102]
[INFO][GP] entropy=4.605 | mu_std=0.0102 | temp=0.95 | top idx [1, 0, 12] probs [0.0102, 0.0102, 0.0102]
epoch [11/100] batch [1/3] loss 1.2517 (1.2517) acc_train 68.7500 (68.7500) acc_test 85.2333 (85.2333) kl_divergence 0.0029 (0.0029) eta 0:00:13
epoch [11/100] batch [2/3] loss 1.3590 (1.3054) acc_train 64.8438 (66.7969) acc_test 84.0162 (84.6247) kl_divergence 0.0031 (0.0030) eta 0:00:12
epoch [11/100] batch [3/3] loss 1.4407 (1.3505) acc_train 63.2812 (65.6250) acc_test 85.5578 (84.9358) kl_divergence 0.0033 (0.0031) eta 0:00:12
[INFO][GP] entropy=4.605 | mu_std=0.0110 | temp=0.95 | top idx [1, 0, 12] probs [0.0103, 0.0102, 0.0102]
[INFO][GP] entropy=4.605 | mu_std=0.0117 | temp=0.94 | top idx [1, 90, 0] probs [0.0103, 0.0103, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0125 | temp=0.94 | top idx [90, 59, 0] probs [0.0103, 0.0103, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0132 | temp=0.93 | top idx [90, 59, 0] probs [0.0104, 0.0103, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0139 | temp=0.92 | top idx [90, 59, 0] probs [0.0104, 0.0104, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0146 | temp=0.92 | top idx [90, 59, 0] probs [0.0105, 0.0104, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0153 | temp=0.91 | top idx [90, 59, 0] probs [0.0105, 0.0104, 0.0103]
[INFO][GP] entropy=4.605 | mu_std=0.0159 | temp=0.91 | top idx [90, 59, 0] probs [0.0105, 0.0104, 0.0104]
[INFO][GP] entropy=4.605 | mu_std=0.0166 | temp=0.90 | top idx [90, 59, 0] probs [0.0105, 0.0104, 0.0104]
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] entropy=4.605 | mu_std=0.0173 | temp=0.89 | top idx [90, 59, 0] probs [0.0105, 0.0104, 0.0104]
epoch [21/100] batch [1/3] loss 1.2619 (1.2619) acc_train 63.2812 (63.2812) acc_test 85.3550 (85.3550) kl_divergence 0.0097 (0.0097) eta 0:00:12
epoch [21/100] batch [2/3] loss 1.3365 (1.2992) acc_train 62.5000 (62.8906) acc_test 84.2596 (84.8073) kl_divergence 0.0100 (0.0098) eta 0:00:11
epoch [21/100] batch [3/3] loss 1.4813 (1.3599) acc_train 57.0312 (60.9375) acc_test 84.6653 (84.7600) kl_divergence 0.0102 (0.0100) eta 0:00:11
[INFO][GP] entropy=4.605 | mu_std=0.0181 | temp=0.89 | top idx [90, 59, 0] probs [0.0106, 0.0104, 0.0104]
[INFO][GP] entropy=4.605 | mu_std=0.0188 | temp=0.88 | top idx [90, 59, 0] probs [0.0106, 0.0105, 0.0104]
[INFO][GP] entropy=4.605 | mu_std=0.0194 | temp=0.88 | top idx [90, 59, 0] probs [0.0107, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0200 | temp=0.87 | top idx [90, 0, 59] probs [0.0107, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0205 | temp=0.87 | top idx [90, 0, 59] probs [0.0107, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0211 | temp=0.86 | top idx [90, 0, 59] probs [0.0108, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0215 | temp=0.86 | top idx [90, 0, 59] probs [0.0108, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0219 | temp=0.85 | top idx [90, 0, 59] probs [0.0108, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0223 | temp=0.85 | top idx [90, 0, 59] probs [0.0108, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0226 | temp=0.84 | top idx [90, 0, 59] probs [0.0107, 0.0105, 0.0105]
epoch [31/100] batch [1/3] loss 1.4699 (1.4699) acc_train 59.3750 (59.3750) acc_test 84.9493 (84.9493) kl_divergence 0.0156 (0.0156) eta 0:00:10
epoch [31/100] batch [2/3] loss 1.5380 (1.5040) acc_train 57.0312 (58.2031) acc_test 84.1379 (84.5436) kl_divergence 0.0158 (0.0157) eta 0:00:10
epoch [31/100] batch [3/3] loss 1.1024 (1.3701) acc_train 69.5312 (61.9792) acc_test 85.3144 (84.8005) kl_divergence 0.0159 (0.0158) eta 0:00:09
[INFO][GP] entropy=4.605 | mu_std=0.0230 | temp=0.84 | top idx [90, 0, 59] probs [0.0107, 0.0105, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0234 | temp=0.83 | top idx [90, 0, 59] probs [0.0108, 0.0106, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0238 | temp=0.82 | top idx [90, 0, 59] probs [0.0108, 0.0106, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0241 | temp=0.82 | top idx [90, 0, 59] probs [0.0108, 0.0106, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0245 | temp=0.82 | top idx [90, 0, 59] probs [0.0108, 0.0107, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0249 | temp=0.81 | top idx [90, 0, 59] probs [0.0108, 0.0107, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0252 | temp=0.81 | top idx [90, 0, 59] probs [0.0108, 0.0107, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0256 | temp=0.80 | top idx [90, 0, 59] probs [0.0109, 0.0107, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0259 | temp=0.80 | top idx [90, 0, 59] probs [0.0109, 0.0108, 0.0105]
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] entropy=4.605 | mu_std=0.0262 | temp=0.79 | top idx [90, 0, 59] probs [0.0109, 0.0108, 0.0105]
epoch [41/100] batch [1/3] loss 1.2197 (1.2197) acc_train 68.7500 (68.7500) acc_test 86.6531 (86.6531) kl_divergence 0.0195 (0.0195) eta 0:00:09
epoch [41/100] batch [2/3] loss 1.0832 (1.1515) acc_train 69.5312 (69.1406) acc_test 85.9635 (86.3083) kl_divergence 0.0196 (0.0195) eta 0:00:08
epoch [41/100] batch [3/3] loss 1.2766 (1.1932) acc_train 67.1875 (68.4896) acc_test 83.8540 (85.4902) kl_divergence 0.0197 (0.0196) eta 0:00:08
[INFO][GP] entropy=4.605 | mu_std=0.0265 | temp=0.79 | top idx [90, 0, 59] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0268 | temp=0.78 | top idx [90, 0, 59] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0270 | temp=0.78 | top idx [90, 0, 59] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0272 | temp=0.77 | top idx [90, 0, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0274 | temp=0.77 | top idx [90, 0, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0276 | temp=0.77 | top idx [90, 0, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0277 | temp=0.76 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0279 | temp=0.76 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0280 | temp=0.75 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0281 | temp=0.75 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
epoch [51/100] batch [1/3] loss 1.5491 (1.5491) acc_train 55.4688 (55.4688) acc_test 87.0183 (87.0183) kl_divergence 0.0217 (0.0217) eta 0:00:07
epoch [51/100] batch [2/3] loss 1.5122 (1.5306) acc_train 59.3750 (57.4219) acc_test 85.6389 (86.3286) kl_divergence 0.0217 (0.0217) eta 0:00:07
epoch [51/100] batch [3/3] loss 1.3856 (1.4823) acc_train 67.1875 (60.6771) acc_test 85.5172 (86.0581) kl_divergence 0.0218 (0.0217) eta 0:00:06
[INFO][GP] entropy=4.605 | mu_std=0.0283 | temp=0.74 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0284 | temp=0.74 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0285 | temp=0.74 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0286 | temp=0.73 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0287 | temp=0.73 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0287 | temp=0.73 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0288 | temp=0.72 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0289 | temp=0.72 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0289 | temp=0.71 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] entropy=4.605 | mu_std=0.0290 | temp=0.71 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
epoch [61/100] batch [1/3] loss 1.0706 (1.0706) acc_train 71.0938 (71.0938) acc_test 85.3144 (85.3144) kl_divergence 0.0227 (0.0227) eta 0:00:06
epoch [61/100] batch [2/3] loss 1.3629 (1.2167) acc_train 64.8438 (67.9688) acc_test 85.1521 (85.2333) kl_divergence 0.0227 (0.0227) eta 0:00:05
epoch [61/100] batch [3/3] loss 1.3617 (1.2651) acc_train 61.7188 (65.8854) acc_test 84.8682 (85.1116) kl_divergence 0.0227 (0.0227) eta 0:00:05
[INFO][GP] entropy=4.605 | mu_std=0.0290 | temp=0.71 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0291 | temp=0.71 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0291 | temp=0.70 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0292 | temp=0.70 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0292 | temp=0.70 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0293 | temp=0.69 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0293 | temp=0.69 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0293 | temp=0.69 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0294 | temp=0.69 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0294 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
epoch [71/100] batch [1/3] loss 1.4808 (1.4808) acc_train 58.5938 (58.5938) acc_test 84.2596 (84.2596) kl_divergence 0.0231 (0.0231) eta 0:00:04
epoch [71/100] batch [2/3] loss 1.6269 (1.5538) acc_train 60.9375 (59.7656) acc_test 85.5578 (84.9087) kl_divergence 0.0232 (0.0231) eta 0:00:04
epoch [71/100] batch [3/3] loss 1.6267 (1.5781) acc_train 60.9375 (60.1562) acc_test 84.6653 (84.8276) kl_divergence 0.0232 (0.0232) eta 0:00:04
[INFO][GP] entropy=4.605 | mu_std=0.0294 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0294 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0295 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0295 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0295 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0108, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0295 | temp=0.68 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[DEBUG] modes -> adapter.train(): True gp.train(): True vision.eval(): True text.eval(): True
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
epoch [81/100] batch [1/3] loss 1.2047 (1.2047) acc_train 66.4062 (66.4062) acc_test 84.1785 (84.1785) kl_divergence 0.0233 (0.0233) eta 0:00:03
epoch [81/100] batch [2/3] loss 1.3991 (1.3019) acc_train 63.2812 (64.8438) acc_test 84.4625 (84.3205) kl_divergence 0.0233 (0.0233) eta 0:00:02
epoch [81/100] batch [3/3] loss 1.4427 (1.3488) acc_train 62.5000 (64.0625) acc_test 86.3286 (84.9899) kl_divergence 0.0233 (0.0233) eta 0:00:02
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0296 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
epoch [91/100] batch [1/3] loss 1.2458 (1.2458) acc_train 64.8438 (64.8438) acc_test 86.1663 (86.1663) kl_divergence 0.0234 (0.0234) eta 0:00:01
epoch [91/100] batch [2/3] loss 1.2710 (1.2584) acc_train 69.5312 (67.1875) acc_test 86.4909 (86.3286) kl_divergence 0.0234 (0.0234) eta 0:00:01
epoch [91/100] batch [3/3] loss 1.4906 (1.3358) acc_train 58.5938 (64.3229) acc_test 85.8824 (86.1799) kl_divergence 0.0234 (0.0234) eta 0:00:01
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
[INFO][GP] entropy=4.605 | mu_std=0.0297 | temp=0.67 | top idx [0, 90, 1] probs [0.0109, 0.0109, 0.0105]
epoch [100/100] batch [1/3] loss 1.4862 (1.4862) acc_train 61.7188 (61.7188) acc_test 85.0710 (85.0710) kl_divergence 0.0234 (0.0234) eta 0:00:00
epoch [100/100] batch [2/3] loss 1.5826 (1.5344) acc_train 58.5938 (60.1562) acc_test 85.5172 (85.2941) kl_divergence 0.0234 (0.0234) eta 0:00:00
epoch [100/100] batch [3/3] loss 1.4392 (1.5027) acc_train 60.1562 (60.1562) acc_test 85.2738 (85.2874) kl_divergence 0.0234 (0.0234) eta 0:00:00
Checkpoint saved to output/gp_test_v3/caltech101/GP_linear_VP_ZSInit_noneConstraint_4shots_10templates/seed1/adapter/model.pth.tar-100
Finish training
Elapsed: 0:00:22
Evaluate on the *test* set
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:03,  3.35s/it] 10%|â–ˆ         | 2/20 [00:03<00:26,  1.47s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:14,  1.16it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:09,  1.72it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:06,  2.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:04,  3.01it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:03,  3.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:02,  4.22it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:02,  3.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:04<00:02,  4.30it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:01,  4.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:05<00:01,  5.29it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:05<00:01,  5.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:05<00:01,  5.94it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:05<00:00,  6.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:05<00:00,  6.31it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:05<00:00,  6.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:06<00:00,  6.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:06<00:00,  6.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.14it/s]
=> result
* total: 2,465
* correct: 2,070
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 82.0%
